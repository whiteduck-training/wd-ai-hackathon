{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: Building an AI Knowledge System\n",
    "## Capstone Project: Intelligent Knowledge Graph Assistant\n",
    "\n",
    "This module combines everything we've learned about Semantic Kernel to build an intelligent system that:\n",
    "- Extracts information from text using AI\n",
    "- Stores it in a knowledge graph\n",
    "- Uses agents to manage and query the knowledge\n",
    "- Employs processes to handle complex operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "from typing import List, Dict, Any\n",
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.memory import VolatileMemoryStore\n",
    "from semantic_kernel.planners import SequentialPlanner\n",
    "\n",
    "# Graph database\n",
    "from gqlalchemy import Memgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Core Knowledge Graph System\n",
    "\n",
    "First, let's create our base system that handles the graph database operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeType(str, Enum):\n",
    "    CONCEPT = \"CONCEPT\"\n",
    "    ENTITY = \"ENTITY\"\n",
    "    EVENT = \"EVENT\"\n",
    "    FACT = \"FACT\"\n",
    "\n",
    "class RelationType(str, Enum):\n",
    "    IS_A = \"IS_A\"\n",
    "    HAS_PROPERTY = \"HAS_PROPERTY\"\n",
    "    RELATED_TO = \"RELATED_TO\"\n",
    "    HAPPENED_AT = \"HAPPENED_AT\"\n",
    "    PARTICIPATED_IN = \"PARTICIPATED_IN\"\n",
    "\n",
    "class KnowledgeGraphDB:\n",
    "    def __init__(self):\n",
    "        # Initialize Memgraph connection\n",
    "        self.db = Memgraph()\n",
    "        self._setup_schema()\n",
    "    \n",
    "    def _setup_schema(self):\n",
    "        # Create indexes and constraints\n",
    "        self.db.execute(\n",
    "            \"CREATE INDEX ON :CONCEPT(name);\",\n",
    "            \"CREATE INDEX ON :ENTITY(name);\",\n",
    "            \"CREATE CONSTRAINT ON (n:CONCEPT) ASSERT n.name IS UNIQUE;\",\n",
    "        )\n",
    "    \n",
    "    async def add_node(self, name: str, node_type: NodeType, properties: dict = None):\n",
    "        query = f\"\"\"\n",
    "        CREATE (n:{node_type} {{name: $name}})\n",
    "        SET n += $properties\n",
    "        RETURN n\n",
    "        \"\"\"\n",
    "        result = self.db.execute(query, {'name': name, 'properties': properties or {}})\n",
    "        return result.single()['n']\n",
    "    \n",
    "    async def add_relation(self, from_node: str, to_node: str, relation_type: RelationType, properties: dict = None):\n",
    "        query = f\"\"\"\n",
    "        MATCH (a), (b)\n",
    "        WHERE a.name = $from_name AND b.name = $to_name\n",
    "        CREATE (a)-[r:{relation_type} $properties]->(b)\n",
    "        RETURN r\n",
    "        \"\"\"\n",
    "        result = self.db.execute(\n",
    "            query, \n",
    "            {\n",
    "                'from_name': from_node, \n",
    "                'to_name': to_node,\n",
    "                'properties': properties or {}\n",
    "            }\n",
    "        )\n",
    "        return result.single()['r']\n",
    "    \n",
    "    async def query_subgraph(self, start_node: str, depth: int = 2):\n",
    "        query = f\"\"\"\n",
    "        MATCH path = (start)-[*1..{depth}]-(related)\n",
    "        WHERE start.name = $name\n",
    "        RETURN path\n",
    "        \"\"\"\n",
    "        return self.db.execute(query, {'name': start_node})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Knowledge Extraction Agents\n",
    "\n",
    "Now let's create agents that can extract knowledge from text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeExtraction(BaseModel):\n",
    "    \"\"\"Structure for extracted knowledge\"\"\"\n",
    "    concepts: List[str] = Field(description=\"Main concepts identified\")\n",
    "    entities: List[str] = Field(description=\"Named entities found\")\n",
    "    relationships: List[Dict[str, str]] = Field(description=\"Relationships between concepts/entities\")\n",
    "    facts: List[str] = Field(description=\"Factual statements extracted\")\n",
    "\n",
    "class KnowledgeExtractionAgent:\n",
    "    def __init__(self, kernel: sk.Kernel):\n",
    "        # Create semantic function for extraction\n",
    "        self.extract_knowledge = kernel.create_semantic_function(\n",
    "            prompt_template=\"\"\"\n",
    "            Analyze the following text and extract key knowledge elements.\n",
    "            Structure the knowledge into:\n",
    "            - Main concepts\n",
    "            - Named entities\n",
    "            - Relationships between them\n",
    "            - Key facts\n",
    "            \n",
    "            Text: {{$input}}\n",
    "            \n",
    "            Respond in the following JSON format:\n",
    "            {\n",
    "                \"concepts\": [\"concept1\", \"concept2\"],\n",
    "                \"entities\": [\"entity1\", \"entity2\"],\n",
    "                \"relationships\": [\n",
    "                    {\"from\": \"entity1\", \"to\": \"concept1\", \"type\": \"IS_A\"},\n",
    "                    {\"from\": \"entity1\", \"to\": \"entity2\", \"type\": \"RELATED_TO\"}\n",
    "                ],\n",
    "                \"facts\": [\"fact1\", \"fact2\"]\n",
    "            }\n",
    "            \"\"\",\n",
    "            function_name=\"extract_knowledge\",\n",
    "            description=\"Extracts structured knowledge from text\"\n",
    "        )\n",
    "    \n",
    "    async def process_text(self, text: str) -> KnowledgeExtraction:\n",
    "        # Extract knowledge\n",
    "        result = await self.extract_knowledge.invoke(text)\n",
    "        return KnowledgeExtraction.model_validate_json(str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Knowledge Graph Process\n",
    "\n",
    "Let's create a process to handle the flow of knowledge ingestion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeIngestionState(BaseModel):\n",
    "    text_processed: bool = False\n",
    "    knowledge_extracted: bool = False\n",
    "    nodes_created: List[str] = []\n",
    "    relations_created: List[Dict] = []\n",
    "    errors: List[str] = []\n",
    "\n",
    "class TextProcessingStep(KernelProcessStep[KnowledgeIngestionState]):\n",
    "    def __init__(self, extractor: KnowledgeExtractionAgent):\n",
    "        self.extractor = extractor\n",
    "        super().__init__()\n",
    "    \n",
    "    @kernel_function\n",
    "    async def process_text(self, context: KernelProcessStepContext, text: str):\n",
    "        try:\n",
    "            knowledge = await self.extractor.process_text(text)\n",
    "            self.state.text_processed = True\n",
    "            await context.emit_event(\"KnowledgeExtracted\", knowledge)\n",
    "        except Exception as e:\n",
    "            self.state.errors.append(f\"Text processing failed: {str(e)}\")\n",
    "            await context.emit_event(\"ProcessingError\")\n",
    "\n",
    "class GraphUpdateStep(KernelProcessStep[KnowledgeIngestionState]):\n",
    "    def __init__(self, graph_db: KnowledgeGraphDB):\n",
    "        self.db = graph_db\n",
    "        super().__init__()\n",
    "    \n",
    "    @kernel_function\n",
    "    async def update_graph(self, context: KernelProcessStepContext, knowledge: KnowledgeExtraction):\n",
    "        try:\n",
    "            # Add concepts and entities\n",
    "            for concept in knowledge.concepts:\n",
    "                node = await self.db.add_node(concept, NodeType.CONCEPT)\n",
    "                self.state.nodes_created.append(node.name)\n",
    "            \n",
    "            for entity in knowledge.entities:\n",
    "                node = await self.db.add_node(entity, NodeType.ENTITY)\n",
    "                self.state.nodes_created.append(node.name)\n",
    "            \n",
    "            # Add relationships\n",
    "            for rel in knowledge.relationships:\n",
    "                relation = await self.db.add_relation(\n",
    "                    rel[\"from\"], \n",
    "                    rel[\"to\"], \n",
    "                    RelationType[rel[\"type\"]]\n",
    "                )\n",
    "                self.state.relations_created.append({\n",
    "                    \"from\": rel[\"from\"],\n",
    "                    \"to\": rel[\"to\"],\n",
    "                    \"type\": rel[\"type\"]\n",
    "                })\n",
    "            \n",
    "            self.state.knowledge_extracted = True\n",
    "            await context.emit_event(\"GraphUpdated\")\n",
    "        except Exception as e:\n",
    "            self.state.errors.append(f\"Graph update failed: {str(e)}\")\n",
    "            await context.emit_event(\"UpdateError\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Query Agent System\n",
    "\n",
    "Now let's create agents that can query and reason about the knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryResult(BaseModel):\n",
    "    \"\"\"Structure for query results\"\"\"\n",
    "    answer: str = Field(description=\"Direct answer to the query\")\n",
    "    explanation: str = Field(description=\"Explanation of how the answer was derived\")\n",
    "    confidence: float = Field(description=\"Confidence score for the answer\")\n",
    "    supporting_facts: List[str] = Field(description=\"Facts from the graph that support the answer\")\n",
    "\n",
    "class KnowledgeQueryAgent:\n",
    "    def __init__(self, kernel: sk.Kernel, graph_db: KnowledgeGraphDB):\n",
    "        self.db = graph_db\n",
    "        self.query_function = kernel.create_semantic_function(\n",
    "            prompt_template=\"\"\"\n",
    "            Answer the following question using the provided knowledge graph context.\n",
    "            \n",
    "            Question: {{$question}}\n",
    "            \n",
    "            Context from knowledge graph:\n",
    "            {{$context}}\n",
    "            \n",
    "            Respond in the following JSON format:\n",
    "            {\n",
    "                \"answer\": \"direct answer\",\n",
    "                \"explanation\": \"how you arrived at the answer\",\n",
    "                \"confidence\": 0.95,\n",
    "                \"supporting_facts\": [\"fact1\", \"fact2\"]\n",
    "            }\n",
    "            \n",
    "            Only use information that is directly supported by the context.\n",
    "            If you're unsure, indicate lower confidence.\n",
    "            \"\"\",\n",
    "            function_name=\"query_knowledge\",\n",
    "            description=\"Queries knowledge graph to answer questions\"\n",
    "        )\n",
    "    \n",
    "    async def query(self, question: str) -> QueryResult:\n",
    "        # Extract key terms from question\n",
    "        key_terms = await self._extract_key_terms(question)\n",
    "        \n",
    "        # Query graph for relevant context\n",
    "        context = []\n",
    "        for term in key_terms:\n",
    "            subgraph = await self.db.query_subgraph(term)\n",
    "            context.extend(self._format_subgraph(subgraph))\n",
    "        \n",
    "        # Get answer using context\n",
    "        result = await self.query_function.invoke(\n",
    "            question=question,\n",
    "            context=\"\\n\".join(context)\n",
    "        )\n",
    "        \n",
    "        return QueryResult.model_validate_json(str(result))\n",
    "    \n",
    "    async def _extract_key_terms(self, question: str) -> List[str]:\n",
    "        # Implementation to extract key terms from question\n",
    "        pass\n",
    "    \n",
    "    def _format_subgraph(self, subgraph) -> List[str]:\n",
    "        # Implementation to format subgraph into readable context\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Putting It All Together\n",
    "\n",
    "Here's how to use the complete system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # Initialize Kernel\n",
    "    kernel = sk.Kernel()\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(\n",
    "            service_id=\"chat-gpt\",\n",
    "            ai_model_id=\"gpt-4\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Initialize components\n",
    "    graph_db = KnowledgeGraphDB()\n",
    "    extractor = KnowledgeExtractionAgent(kernel)\n",
    "    query_agent = KnowledgeQueryAgent(kernel, graph_db)\n",
    "    \n",
    "    # Create ingestion process\n",
    "    from semantic_kernel.processes.process_builder import ProcessBuilder\n",
    "    \n",
    "    process = ProcessBuilder(\"KnowledgeIngestion\")\n",
    "    text_step = process.add_step(TextProcessingStep(extractor))\n",
    "    graph_step = process.add_step(GraphUpdateStep(graph_db))\n",
    "    \n",
    "    # Configure process flow\n",
    "    process.on_input_event(\"StartIngestion\").send_event_to(text_step)\n",
    "    text_step.on_event(\"KnowledgeExtracted\").send_event_to(graph_step)\n",
    "    text_step.on_event(\"ProcessingError\").stop_process()\n",
    "    graph_step.on_event(\"UpdateError\").stop_process()\n",
    "    \n",
    "    # Example usage\n",
    "    sample_text = \"\"\"\n",
    "    The Python programming language was created by Guido van Rossum and was first released in 1991. \n",
    "    Python is known for its simple syntax and readability, which makes it popular among beginners. \n",
    "    It supports multiple programming paradigms, including procedural, object-oriented, and functional programming.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ingest knowledge\n",
    "    kernel_process = process.build()\n",
    "    result = await start(\n",
    "        process=kernel_process,\n",
    "        kernel=kernel,\n",
    "        initial_event=KernelProcessEvent(id=\"StartIngestion\", data=sample_text)\n",
    "    )\n",
    "    \n",
    "    # Query knowledge\n",
    "    questions = [\n",
    "        \"Who created Python?\",\n",
    "        \"What are the main characteristics of Python?\",\n",
    "        \"When was Python first released?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        result = await query_agent.query(question)\n",
    "        print(f\"\\nQ: {question}\")\n",
    "        print(f\"A: {result.answer}\")\n",
    "        print(f\"Confidence: {result.confidence}\")\n",
    "        print(\"Supporting facts:\")\n",
    "        for fact in result.supporting_facts:\n",
    "            print(f\"- {fact}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Part 6: Enhancements and Extensions\n",
    "\n",
    "1. Add document processing for different file types\n",
    "2. Implement fact verification using external sources\n",
    "3. Add temporal reasoning capabilities\n",
    "4. Create visualization tools for the knowledge graph\n",
    "5. Implement belief system (certainty scores for facts)\n",
    "6. Add automatic knowledge refreshing\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Knowledge Quality**\n",
    "   - Validate extracted information\n",
    "   - Track knowledge provenance\n",
    "   - Handle contradictions\n",
    "   - Update outdated information\n",
    "\n",
    "2. **Performance**\n",
    "   - Index frequently accessed nodes\n",
    "   - Cache common query patterns\n",
    "   - Batch graph updates\n",
    "   - Implement query optimization\n",
    "\n",
    "3. **Error Handling**\n",
    "   - Validate input text\n",
    "   - Handle extraction failures\n",
    "   - Manage graph inconsistencies\n",
    "   - Track failed queries\n",
    "\n",
    "4. **Security**\n",
    "   - Validate knowledge sources\n",
    "   - Implement access control\n",
    "   - Audit knowledge changes\n",
    "   - Secure sensitive information\n",
    "\n",
    "### Resources\n",
    "- [Semantic Kernel Documentation](https://learn.microsoft.com/semantic-kernel/)\n",
    "- [MemGraph Documentation](https://memgraph.com/docs)\n",
    "- [Graph Database Patterns](https://neo4j.com/developer/cypher/guide-sql-to-cypher/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
